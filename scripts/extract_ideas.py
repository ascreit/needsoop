#!/usr/bin/env python3
"""
ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚¤ãƒ‡ã‚¢ã«ãªã‚‹å›°ã‚Šã”ã¨æŠ•ç¨¿ã‚’æŠ½å‡ºã—ã€
ãƒ‹ãƒ¼ã‚ºã®å¤šã•ã‚’åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
from pathlib import Path
import re
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
from src.db.chroma import PostStore
from src.analysis.clustering import PostClusterer
from sentence_transformers import SentenceTransformer


# ã‚¹ãƒ‘ãƒ ãƒ»ãƒœãƒƒãƒˆé™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
SPAM_PATTERNS = [
    r'#ì„¹ë¸”|#ì„¹íŠ¸|#ì•¼ë°©|#ê²Œì´|#ì˜¤í”„',  # éŸ“å›½èªå‡ºä¼šã„ç³»
    r'DMM.*è²©å£²ä¸­',  # DMMã‚¢ãƒ•ã‚£ãƒªã‚¨ã‚¤ãƒˆ
    r'www\.dmm\.com',
    r'plamofigure\.com',
    r'è²©å£²ä¾¡æ ¼:\d+å††',
    r'#PR\s*$',
    r'#ãƒ•ã‚£ã‚®ãƒ¥ã‚¢\s*#PR',
    r'äºˆç´„å—ä»˜ä¸­ï¼',
    r'é€šè²©ã§è²©å£²ä¸­',
    r'â—‹â—‹â—‹â—‹â—‹â—‹',  # æ„å‘³ä¸æ˜ãªç¹°ã‚Šè¿”ã—
    r'ã‚¦ã‚ªã‚ªã‚ªã‚ªã‚ª',
    r'https://al\.dmm\.com',
    r'#AIart',
    r'#mlsb',  # å‰µä½œã‚¿ã‚°
    r'ğŸŒ¸ã€Œ|ğŸ›’ã€Œ|ğŸ›’ãã‚“',  # å‰µä½œå°èª¬ã®ä¼šè©±å½¢å¼
    r'youtube\.com|youtu\.be',  # YouTubeå®£ä¼
    r'netkeiba',  # ç«¶é¦¬ãƒ‹ãƒ¥ãƒ¼ã‚¹
    r'ã‚³ãƒŸãƒ†ã‚£ã‚¢|COMITIA',  # åŒäººã‚¤ãƒ™ãƒ³ãƒˆå®£ä¼
    r'å…¨è‚¯å®šbot',  # bot
    r'ã‚­ãƒ£ãƒ©ãƒ‡ã‚¶|ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼',  # ã‚ªã‚¿ã‚¯å‰µä½œ
    r'äºŒæ¬¡å‰µä½œ',
    r'åŒäºº|é ’å¸ƒ',
    r'æ¨ã—|ã‚ªã‚¿ã‚¯',
    r'ã‚¢ãƒ‹ãƒ¡|æ¼«ç”»|ãƒãƒ³ã‚¬',
    r'ã‚²ãƒ¼ãƒ |ã‚¬ãƒãƒ£|ãƒ”ãƒƒã‚¯ã‚¢ãƒƒãƒ—',
    r'ãƒ•ã‚£ã‚®ãƒ¥ã‚¢',
    r'ãƒ‰ç™–|æ€§ç™–',
    r'ãƒ¡ãƒ†ã‚£ã‚¹|ã‚¬ãƒãƒ£çŸ³',  # ã‚½ã‚·ãƒ£ã‚²
    r'æ–°åˆŠ.*å†Š',  # åŒäººèªŒ
    r'ãŠå“æ›¸ã',
]

# å›°ã‚Šã”ã¨ãƒ»ä¸æº€ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
COMPLAINT_KEYWORDS = [
    'å›°', 'é¢å€’', 'ã‚ã‚“ã©ã†', 'ã‚ã‚“ã©ãã•', 'ã¤ã‚‰ã„', 'è¾›ã„', 'ã—ã‚“ã©ã„',
    'ç–²ã‚Œ', 'ã¤ã‹ã‚Œ', 'å¤§å¤‰', 'ãŸã„ã¸ã‚“', 'é›£ã—', 'ã‚€ãšã‹ã—',
    'ä¸æº€', 'ä¸ä¾¿', 'å«Œ', 'ã„ã‚„', 'ã‚„ã ', 'ç„¡ç†', 'ã‚€ã‚Š',
    'æ‚©', 'ãªã‚„', 'å•é¡Œ', 'ãƒˆãƒ©ãƒ–ãƒ«', 'ã‚¤ãƒ©ã‚¤ãƒ©', 'ã‚¹ãƒˆãƒ¬ã‚¹',
    'åˆ†ã‹ã‚‰ãªã„', 'ã‚ã‹ã‚‰ãªã„', 'ã‚ã‹ã‚“ãªã„', 'ã§ããªã„', 'å‡ºæ¥ãªã„',
    'æ¬²ã—ã„', 'ã»ã—ã„', 'ã—ã¦ã»ã—ã„', 'ãªã‚“ã¨ã‹',
    'é«˜ã„', 'é«˜ã™ã', 'æ™‚é–“ãŒãªã„', 'æ™‚é–“ã‹ã‹ã‚‹', 'å¾…ãŸ',
    'ãƒã‚°', 'ã‚¨ãƒ©ãƒ¼', 'å‹•ã‹ãªã„', 'å£Šã‚Œ', 'ä½¿ãˆãªã„', 'è½ã¡',
]

# ã‚ªã‚¿ã‚¯ãƒ»å‰µä½œç³»ã®è¿½åŠ é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
OTAKU_PATTERNS = [
    r'æã„[ãŸã¦]', r'æ›¸ã„[ãŸã¦]',  # å‰µä½œæ´»å‹•
    r'ã€†åˆ‡|ç· ã‚åˆ‡ã‚Š|å…¥ç¨¿',
    r'åŸç¨¿',
    r'ã‚¤ãƒ™ãƒ³ãƒˆ|å³å£²ä¼š',
    r'çµµå¸«|ä½œå®¶',
    r'æœ¬.*å‡º[ã™ã—]',
    r'ã‚µãƒ¼ã‚¯ãƒ«',
    r'ã‚¹ãƒšãƒ¼ã‚¹',
]


def is_spam(text: str) -> bool:
    """ã‚¹ãƒ‘ãƒ ãƒ»ãƒœãƒƒãƒˆæŠ•ç¨¿ã‹ã©ã†ã‹åˆ¤å®š"""
    for pattern in SPAM_PATTERNS:
        if re.search(pattern, text):
            return True
    return False


def is_otaku_content(text: str) -> bool:
    """ã‚ªã‚¿ã‚¯ãƒ»å‰µä½œç³»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹åˆ¤å®š"""
    for pattern in OTAKU_PATTERNS:
        if re.search(pattern, text):
            return True
    return False


def has_complaint_keyword(text: str) -> bool:
    """å›°ã‚Šã”ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ã‹åˆ¤å®š"""
    for keyword in COMPLAINT_KEYWORDS:
        if keyword in text:
            return True
    return False


def is_japanese(text: str) -> bool:
    """æ—¥æœ¬èªã‚’å«ã‚€ã‹åˆ¤å®šï¼ˆä¸­å›½èªã‚’é™¤å¤–ï¼‰"""
    # ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãŒä¸€å®šæ•°ã‚ã‚‹ã‹ï¼ˆæ¼¢å­—ã®ã¿ã ã¨ä¸­å›½èªã®å¯èƒ½æ€§ï¼‰
    hiragana = len(re.findall(r'[\u3040-\u309F]', text))
    katakana = len(re.findall(r'[\u30A0-\u30FF]', text))
    return (hiragana + katakana) >= 3


def main():
    store = PostStore(persist_directory=Path("data/chroma"))
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

    # è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã§æ¤œç´¢
    queries = [
        "å›°ã£ã¦ã„ã‚‹ å¤§å¤‰ é¢å€’ãã•ã„",
        "ã€œã§ããªãã¦å›°ã‚‹",
        "ä¸ä¾¿ ä½¿ã„ã«ãã„ åˆ†ã‹ã‚Šã«ãã„",
        "ç–²ã‚ŒãŸ ã—ã‚“ã©ã„ ã¤ã‚‰ã„",
        "ã“ã†ã ã£ãŸã‚‰ã„ã„ã®ã« æ¬²ã—ã„",
        "å•é¡Œ ãƒˆãƒ©ãƒ–ãƒ« è§£æ±ºã—ãŸã„",
    ]

    all_results = {}

    print("æ¤œç´¢ä¸­...")
    for query in queries:
        query_embedding = model.encode(query).tolist()
        results = store.search(query_embedding=query_embedding, n_results=2000)

        for post, score in results:
            if post.id not in all_results or score < all_results[post.id][1]:
                all_results[post.id] = (post, score)

    print(f"æ¤œç´¢çµæœï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰: {len(all_results)}ä»¶")

    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    filtered = []
    for post, score in all_results.values():
        # ã‚¹ã‚³ã‚¢é–¾å€¤ï¼ˆä½ã„ã»ã©é¡ä¼¼åº¦é«˜ã„ï¼‰
        if score > 0.85:
            continue

        # ã‚¹ãƒ‘ãƒ é™¤å¤–
        if is_spam(post.text):
            continue

        # ã‚ªã‚¿ã‚¯ãƒ»å‰µä½œç³»é™¤å¤–
        if is_otaku_content(post.text):
            continue

        # æ—¥æœ¬èªã®ã¿
        if not is_japanese(post.text):
            continue

        # å›°ã‚Šã”ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€ï¼ˆã‚¹ã‚³ã‚¢ãŒä½ã„ï¼é¡ä¼¼åº¦é«˜ã„å ´åˆã¯å…é™¤ï¼‰
        if score > 0.6 and not has_complaint_keyword(post.text):
            continue

        filtered.append((post, score))

    print(f"ãƒ•ã‚£ãƒ«ã‚¿å¾Œ: {len(filtered)}ä»¶")

    if len(filtered) < 10:
        print("å›°ã‚Šã”ã¨æŠ•ç¨¿ãŒå°‘ãªã™ãã¾ã™ã€‚")
        return

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ç”¨ã®embeddingå–å¾—
    print("\nã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ä¸­...")
    embeddings_dict = store.get_embeddings()

    posts_with_emb = []
    embeddings = []
    for post, score in filtered:
        if post.id in embeddings_dict:
            posts_with_emb.append((post, score))
            embeddings.append(embeddings_dict[post.id])

    embeddings_array = np.array(embeddings)

    # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
    clusterer = PostClusterer(min_cluster_size=3)
    result = clusterer.fit(embeddings_array)

    print(f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {result.n_clusters}")
    print(f"ãƒã‚¤ã‚º: {result.n_noise}ä»¶")

    # å„ã‚¯ãƒ©ã‚¹ã‚¿ã®æŠ•ç¨¿ã‚’åé›†
    cluster_posts = defaultdict(list)
    for i, (post, score) in enumerate(posts_with_emb):
        cluster_id = result.labels[i]
        if cluster_id != -1:  # ãƒã‚¤ã‚ºä»¥å¤–
            cluster_posts[cluster_id].append((post, score))

    # ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ã‚µã‚¤ã‚ºé †ã«ã‚½ãƒ¼ãƒˆ
    sorted_clusters = sorted(
        cluster_posts.items(),
        key=lambda x: len(x[1]),
        reverse=True
    )

    # çµæœè¡¨ç¤º
    print("\n" + "=" * 80)
    print("ã€ãƒ‹ãƒ¼ã‚ºåˆ†æçµæœã€‘ã‚µã‚¤ã‚ºé †ï¼ˆåŒã˜æ‚©ã¿ã‚’æŒã¤äººãŒå¤šã„é †ï¼‰")
    print("=" * 80)

    for cluster_id, posts in sorted_clusters:
        # ã‚¯ãƒ©ã‚¹ã‚¿ã®æ³¨ç›®åº¦ï¼ˆã„ã„ã­ãƒ»ãƒªãƒã‚¹ãƒˆåˆè¨ˆï¼‰
        total_engagement = sum(p.likes + p.reposts for p, _ in posts)
        avg_engagement = total_engagement / len(posts) if posts else 0

        print(f"\n### ã‚¯ãƒ©ã‚¹ã‚¿ {cluster_id}: {len(posts)}ä»¶ (å¹³å‡ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ: {avg_engagement:.1f})")
        print("-" * 40)

        # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤ºï¼ˆæœ€å¤§5ä»¶ï¼‰
        for i, (post, score) in enumerate(posts[:5], 1):
            text = post.text.replace("\n", " ")[:150]
            engagement = f"[â™¥{post.likes} ğŸ”{post.reposts}]" if post.likes or post.reposts else ""
            print(f"  {i}. {engagement} {text}...")

        if len(posts) > 5:
            print(f"  ... ä»– {len(posts) - 5}ä»¶")

    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ã€ã‚µãƒãƒªãƒ¼ã€‘")
    print("=" * 80)
    print(f"ç·å›°ã‚Šã”ã¨æŠ•ç¨¿æ•°: {len(filtered)}ä»¶")
    print(f"ã‚¯ãƒ©ã‚¹ã‚¿æ•°: {result.n_clusters}")
    print(f"ä¸Šä½3ã‚¯ãƒ©ã‚¹ã‚¿:")
    for i, (cluster_id, posts) in enumerate(sorted_clusters[:3], 1):
        sample_text = posts[0][0].text.replace("\n", " ")[:80]
        print(f"  {i}. {len(posts)}ä»¶ - ä¾‹: {sample_text}...")


if __name__ == "__main__":
    main()
